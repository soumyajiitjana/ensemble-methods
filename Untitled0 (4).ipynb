{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Can we use Bagging for regression problems?\n",
        "\n",
        "Answer: Yes. Bagging works for both classification and regression. For regression, it averages the predictions of multiple base regressors to reduce variance.\n",
        "\n",
        "2. What is the difference between multiple model training and single model training?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Single model training: Only one model is trained; performance depends entirely on that model.\n",
        "\n",
        "Multiple model training (Ensemble): Many models are trained and combined to give a stronger, more stable prediction.\n",
        "\n",
        "3. Explain the concept of feature randomness in Random Forest.\n",
        "\n",
        "Answer: Random Forest selects a random subset of features at each split in every tree. This randomness reduces correlation between trees, improving accuracy and reducing overfitting.\n",
        "\n",
        "4. What is OOB (Out-of-Bag) Score?\n",
        "\n",
        "Answer: OOB Score is the model’s performance measured using the samples not included in the bootstrap training set. It acts like an internal cross-validation score.\n",
        "\n",
        "5. How can you measure the importance of features in a Random Forest model?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Using Gini Importance (mean decrease in impurity).\n",
        "\n",
        "Using Permutation Importance (drop in accuracy when a feature is shuffled).\n",
        "\n",
        "6. Explain the working principle of a Bagging Classifier.\n",
        "\n",
        "Answer:\n",
        "\n",
        "Many bootstrap samples are created from the dataset.\n",
        "\n",
        "A model (e.g., Decision Tree) is trained on each sample.\n",
        "\n",
        "All models predict, and majority voting is used for the final result.\n",
        "\n",
        "7. How do you evaluate a Bagging Classifier’s performance?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Using accuracy, precision, recall, F1-score\n",
        "\n",
        "Using confusion matrix\n",
        "\n",
        "Using OOB score (if enabled)\n",
        "\n",
        "Using cross-validation\n",
        "\n",
        "8. How does a Bagging Regressor work?\n",
        "\n",
        "Answer: It trains multiple regression models on different bootstrap samples and combines them using average of predictions, reducing variance.\n",
        "\n",
        "9. What is the main advantage of ensemble techniques?\n",
        "\n",
        "Answer: They increase accuracy, reduce variance, and give better generalization compared to a single model.\n",
        "\n",
        "10. What is the main challenge of ensemble methods?\n",
        "\n",
        "Answer:\n",
        "\n",
        "They require more computation\n",
        "\n",
        "Harder to interpret\n",
        "\n",
        "May take more memory and time\n",
        "\n",
        "11. Explain the key idea behind ensemble techniques.\n",
        "\n",
        "Answer:\n",
        "Combine multiple weak or strong models to create a more accurate, stable, and robust final model.\n",
        "\n",
        "12. What is a Random Forest Classifier?\n",
        "\n",
        "Answer:\n",
        "An ensemble of multiple decision trees where each tree is trained on random samples and random features, and final prediction is by majority vote.\n",
        "\n",
        "13. What are the main types of ensemble techniques?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Bagging\n",
        "\n",
        "Boosting\n",
        "\n",
        "Stacking\n",
        "\n",
        "Voting\n",
        "\n",
        "14. What is ensemble learning in machine learning?\n",
        "\n",
        "Answer:\n",
        "A technique where multiple models are combined to improve accuracy, stability, and performance.\n",
        "\n",
        "15. When should we avoid using ensemble methods?\n",
        "\n",
        "Answer:\n",
        "\n",
        "When interpretability is important\n",
        "\n",
        "When working with very small datasets\n",
        "\n",
        "When computational resources are limited\n",
        "\n",
        "When a simple model already performs well\n",
        "\n",
        "16. How does Bagging help in reducing overfitting?\n",
        "\n",
        "Answer:\n",
        "Bagging reduces variance by averaging predictions of many independent models, thus preventing any single model from overfitting.\n",
        "\n",
        "17. Why is Random Forest better than a single Decision Tree?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Less overfitting\n",
        "\n",
        "More accurate\n",
        "\n",
        "More stable\n",
        "\n",
        "Uses randomness (reducing correlation between trees)\n",
        "\n",
        "18. What is the role of bootstrap sampling in Bagging?\n",
        "\n",
        "Answer:\n",
        "Bootstrap sampling creates multiple different datasets from the original, allowing each model to learn different patterns and increasing diversity.\n",
        "\n",
        "19. What are some real-world applications of ensemble techniques?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Fraud detection\n",
        "\n",
        "Medical diagnosis\n",
        "\n",
        "Stock market prediction\n",
        "\n",
        "Recommendation systems\n",
        "\n",
        "Customer churn prediction\n",
        "\n",
        "Image recognition\n",
        "\n",
        "20. What is the difference between Bagging and Boosting?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Bagging\tBoosting\n",
        "Reduces variance\tReduces bias\n",
        "Models train independently\tModels train sequentially\n",
        "Uses bootstrap sampling\tEach model focuses on previous errors\n",
        "Less prone to overfitting\tMore prone to overfitting\n",
        "Example: Random Forest\tExample: XGBoost, AdaBoost"
      ],
      "metadata": {
        "id": "2U4u92I5JaJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Bagging Classifier\n",
        "bag = BaggingClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "bag.fit(X_train, y_train)\n",
        "pred = bag.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, pred))\n"
      ],
      "metadata": {
        "id": "QvhlfVL4Jd7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Dataset\n",
        "X, y = load_diabetes(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "bag_reg = BaggingRegressor(\n",
        "    base_estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "bag_reg.fit(X_train, y_train)\n",
        "pred = bag_reg.predict(X_test)\n",
        "\n",
        "print(\"MSE:\", mean_squared_error(y_test, pred))\n"
      ],
      "metadata": {
        "id": "0_NcB_mTKFL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "feature_importances = pd.DataFrame({\n",
        "    \"Feature\": data.feature_names,\n",
        "    \"Importance\": rf.feature_importances_\n",
        "}).sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "print(feature_importances)\n"
      ],
      "metadata": {
        "id": "NWruC6Y2KFTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Dataset\n",
        "X, y = load_diabetes(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Decision Tree\n",
        "dt = DecisionTreeRegressor()\n",
        "dt.fit(X_train, y_train)\n",
        "dt_pred = dt.predict(X_test)\n",
        "\n",
        "# Random Forest\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "rf_pred = rf.predict(X_test)\n",
        "\n",
        "print(\"Decision Tree MSE:\", mean_squared_error(y_test, dt_pred))\n",
        "print(\"Random Forest MSE:\", mean_squared_error(y_test, rf_pred))\n"
      ],
      "metadata": {
        "id": "uli_sM_JKFZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    oob_score=True,\n",
        "    bootstrap=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf.fit(X, y)\n",
        "\n",
        "print(\"OOB Score:\", rf.oob_score_)\n"
      ],
      "metadata": {
        "id": "3sCnn0D3KFeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "bag_svm = BaggingClassifier(\n",
        "    base_estimator=SVC(),\n",
        "    n_estimators=20,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "bag_svm.fit(X_train, y_train)\n",
        "pred = bag_svm.predict(X_test)\n",
        "\n",
        "print(\"Bagging (SVM) Accuracy:\", accuracy_score(y_test, pred))\n"
      ],
      "metadata": {
        "id": "HctNnLC_KFgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Data\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "for n in [10, 50, 100, 200]:\n",
        "    rf = RandomForestClassifier(n_estimators=n, random_state=42)\n",
        "    rf.fit(X_train, y_train)\n",
        "    pred = rf.predict(X_test)\n",
        "    print(f\"{n} Trees Accuracy:\", accuracy_score(y_test, pred))\n"
      ],
      "metadata": {
        "id": "4OmF7jaSKFi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "bag_log = BaggingClassifier(\n",
        "    base_estimator=LogisticRegression(max_iter=2000),\n",
        "    n_estimators=20,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "bag_log.fit(X_train, y_train)\n",
        "pred_proba = bag_log.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"AUC Score:\", roc_auc_score(y_test, pred_proba))\n"
      ],
      "metadata": {
        "id": "O5bYha0GKFoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import pandas as pd\n",
        "\n",
        "# Dataset\n",
        "data = load_diabetes()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "importance = pd.DataFrame({\n",
        "    \"Feature\": data.feature_names,\n",
        "    \"Importance\": rf.feature_importances_\n",
        "}).sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "print(importance)\n"
      ],
      "metadata": {
        "id": "_ldsOgk-KFqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Bagging Model\n",
        "bag = BaggingClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=30,\n",
        "    random_state=42\n",
        ")\n",
        "bag.fit(X_train, y_train)\n",
        "bag_pred = bag.predict(X_test)\n",
        "\n",
        "# Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "rf_pred = rf.predict(X_test)\n",
        "\n",
        "print(\"Bagging Accuracy:\", accuracy_score(y_test, bag_pred))\n",
        "print(\"Random Forest Accuracy:\", accuracy_score(y_test, rf_pred))\n"
      ],
      "metadata": {
        "id": "AEfvM6ouKFtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Load data\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Parameter Grid\n",
        "param_grid = {\n",
        "    \"n_estimators\": [50, 100, 200],\n",
        "    \"max_depth\": [3, 5, 10, None],\n",
        "    \"min_samples_split\": [2, 5, 10]\n",
        "}\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "grid = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Params:\", grid.best_params_)\n",
        "print(\"Best Score:\", grid.best_score_)\n"
      ],
      "metadata": {
        "id": "3hVqcgq5KFvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Data\n",
        "X, y = load_diabetes(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "estimators = {\n",
        "    \"Decision Tree\": DecisionTreeRegressor(),\n",
        "    \"KNN\": KNeighborsRegressor()\n",
        "}\n",
        "\n",
        "for name, estimator in estimators.items():\n",
        "    model = BaggingRegressor(base_estimator=estimator, n_estimators=30, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    pred = model.predict(X_test)\n",
        "    print(name, \"MSE:\", mean_squared_error(y_test, pred))\n"
      ],
      "metadata": {
        "id": "YbrOu8A5KFyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Data\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "pred = rf.predict(X_test)\n",
        "misclassified = np.where(pred != y_test)[0]\n",
        "\n",
        "print(\"Misclassified Indices:\", misclassified)\n",
        "print(\"Total Misclassified:\", len(misclassified))\n"
      ],
      "metadata": {
        "id": "04i9e63CKF0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "\n",
        "# Data\n",
        "X, y = load_wine(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Single Decision Tree\n",
        "dt = DecisionTreeClassifier()\n",
        "dt.fit(X_train, y_train)\n",
        "dt_pred = dt.predict(X_test)\n",
        "\n",
        "# Bagging Classifier\n",
        "bag = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=30, random_state=42)\n",
        "bag.fit(X_train, y_train)\n",
        "bag_pred = bag.predict(X_test)\n",
        "\n",
        "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, dt_pred))\n",
        "print(\"Bagging Accuracy:\", accuracy_score(y_test, bag_pred))\n"
      ],
      "metadata": {
        "id": "TcUxN9vVKF2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load data\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "pred = rf.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, pred)\n",
        "\n",
        "sns.heatmap(cm, annot=True, cmap=\"Blues\")\n",
        "plt.title(\"Random Forest Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EltOJPR7KF45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load data\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "pred = rf.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, pred)\n",
        "\n",
        "sns.heatmap(cm, annot=True, cmap=\"Blues\")\n",
        "plt.title(\"Random Forest Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "sc9uY8SbKF7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Data\n",
        "X, y = load_wine(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Base models\n",
        "estimators = [\n",
        "    ('dt', DecisionTreeClassifier()),\n",
        "    ('svm', SVC(probability=True)),\n",
        "]\n",
        "\n",
        "# Final estimator\n",
        "stack = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=LogisticRegression()\n",
        ")\n",
        "\n",
        "stack.fit(X_train, y_train)\n",
        "pred = stack.predict(X_test)\n",
        "\n",
        "print(\"Stacking Accuracy:\", accuracy_score(y_test, pred))\n"
      ],
      "metadata": {
        "id": "YT2lIdYVKkrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"Feature\": data.feature_names,\n",
        "    \"Importance\": rf.feature_importances_\n",
        "}).sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "print(df.head(5))\n"
      ],
      "metadata": {
        "id": "UrQf1rxyKkzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Data\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "bag = BaggingClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=30,\n",
        "    random_state=42\n",
        ")\n",
        "bag.fit(X_train, y_train)\n",
        "pred = bag.predict(X_test)\n",
        "\n",
        "print(\"Precision:\", precision_score(y_test, pred))\n",
        "print(\"Recall:\", recall_score(y_test, pred))\n",
        "print(\"F1 Score:\", f1_score(y_test, pred))\n"
      ],
      "metadata": {
        "id": "Qcn20MjIKk2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Data\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "for depth in [None, 2, 4, 6, 8, 10]:\n",
        "    rf = RandomForestClassifier(max_depth=depth, random_state=42)\n",
        "    rf.fit(X_train, y_train)\n",
        "    pred = rf.predict(X_test)\n",
        "    print(f\"max_depth={depth} -> Accuracy={accuracy_score(y_test, pred)}\")\n"
      ],
      "metadata": {
        "id": "bT0bWYWyKk6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Data\n",
        "X, y = load_diabetes(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "models = [\n",
        "    (\"Decision Tree\", DecisionTreeRegressor()),\n",
        "    (\"KNN\", KNeighborsRegressor())\n",
        "]\n",
        "\n",
        "for name, estimator in models:\n",
        "    bag = BaggingRegressor(\n",
        "        base_estimator=estimator,\n",
        "        n_estimators=30,\n",
        "        random_state=42\n",
        "    )\n",
        "    bag.fit(X_train, y_train)\n",
        "    pred = bag.predict(X_test)\n",
        "    print(name, \"MSE:\", mean_squared_error(y_test, pred))\n"
      ],
      "metadata": {
        "id": "KNL0ujyVKk9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Data\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "pred_proba = rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test, pred_proba))\n"
      ],
      "metadata": {
        "id": "7vXX4y6UKwtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Data\n",
        "X, y = load_diabetes(return_X_y=True)\n",
        "\n",
        "bag = BaggingRegressor(\n",
        "    base_estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=40,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "cv_scores = cross_val_score(bag, X, y, cv=5, scoring=\"neg_mean_squared_error\")\n",
        "print(\"Cross-Validation MSE Scores:\", -cv_scores)\n",
        "print(\"Mean MSE:\", -cv_scores.mean())\n"
      ],
      "metadata": {
        "id": "BuOPDTE5Kw1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "pred_proba = rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, pred_proba)\n",
        "\n",
        "plt.plot(recall, precision)\n",
        "plt.title(\"Precision–Recall Curve\")\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yQr3n5X8Kw69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Data\n",
        "X, y = load_wine(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "estimators = [\n",
        "    (\"rf\", RandomForestClassifier(n_estimators=50)),\n",
        "]\n",
        "\n",
        "stack = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=LogisticRegression(max_iter=2000)\n",
        ")\n",
        "\n",
        "stack.fit(X_train, y_train)\n",
        "pred = stack.predict(X_test)\n",
        "\n",
        "print(\"Stacking Model Accuracy:\", accuracy_score(y_test, pred))\n"
      ],
      "metadata": {
        "id": "qoXFc9ObKxCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Data\n",
        "X, y = load_diabetes(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "sample_sizes = [0.3, 0.5, 0.7, 1.0]\n",
        "\n",
        "for size in sample_sizes:\n",
        "    bag = BaggingRegressor(\n",
        "        base_estimator=DecisionTreeRegressor(),\n",
        "        n_estimators=30,\n",
        "        max_samples=size,\n",
        "        random_state=42\n",
        "    )\n",
        "    bag.fit(X_train, y_train)\n",
        "    pred = bag.predict(X_test)\n",
        "    print(f\"Bootstrap Sample Size {size} -> MSE: {mean_squared_error(y_test, pred)}\")\n"
      ],
      "metadata": {
        "id": "0Lei_q6FKxHk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}